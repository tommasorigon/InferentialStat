---
title: "Point estimation"
subtitle: "Statistical Inference - PhD EcoStatData"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
page-layout: full
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_A_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/InferentialStat)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---


## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_A.qmd", output = "../code/un_A.R", documentation = 0)
styler:::style_file("../code/un_A.R")
```

::: columns
::: {.column width="30%"}
![](img/target.png)

<!-- *"Pluralitas non est ponenda sine necessitate."* -->

<!-- [William of Ockham]{.grey} -->
:::

::: {.column width="70%"}
- This unit will cover the following [topics]{.orange}:

    - Methods of finding estimators
    - Methods of evaluating estimators
    - Best unbiased estimators
    - Robustness and model misspecification

- The rational behind [point estimation]{.blue} is quite simple: 

- When sampling is from a [population]{.orange} described by a pdf or a pmf $f(\cdot ; \theta)$, knowledge of $\theta$ yields knowledge of the entire population.

- Hence, it is natural to seek a method of finding a good [estimator]{.blue} of the unknown point $\theta$.
:::
:::

# Methods of finding estimators

## Estimator

::: callout-note
#### Point estimator

A [point estimator]{.blue} $\hat{\theta}$ is any function of the random sample $Y_1,\dots,Y_n$, namely
$$
\hat{\theta}(\bm{Y}) = \hat{\theta}(Y_1,\dots,Y_n).
$$
That is, any [statistic]{.orange} is a point estimator.
:::

- In principle, the range of the estimator coincide with that of the parameter, i.e.  $\hat{\theta} : \mathcal{Y} \rightarrow \Theta$, but there are exceptions. 
- An [estimator]{.blue} $\hat{\theta}(Y_1,\dots,Y_n)$ is a function of the sample $Y_1,\dots,Y_n$ and  is a [random variable]{.blue}.

- An [estimate]{.orange} $\hat{\theta}(y_1,\dots,y_n)$ is a function of the realized values $y_1,\dots,y_n$ and is a [number]{.orange}. 

- We will use the notation $\hat{\theta}$ to denote both estimators and estimates whenever its meaning is clear from the context.

## Method of moments

- The [methods of moments]{.blue} is, perhaps, the oldest method of finding point estimators, dating back at least to Karl Pearson in the late 1800s.

- Let $Y_1,\dots,Y_n$ be an iid sample from $f(\cdot; \theta)$ and $\theta = (\theta_1,\dots,\theta_k)$. Moreover, let us define
$$
m_r = \frac{1}{n}\sum_{i=1}^n Y_i^k, \qquad \mu_r(\theta) = \mu_r(\theta_1,\dots,\theta_k) = \mathbb{E}_\theta(Y^r), \qquad r=1,\dots,k.
$$
corresponding to the [population moment]{.blue} $\mu_r(\theta_1,\dots,\theta_k)$ and the [sample moment]{.orange} $m_r$. 

- The method of moments estimator $\hat{\theta}$ is obtained by solving the following system of equations for $(\theta_1,\dots,\theta_k)$:
$$
\begin{aligned}
\mu_1(\theta_1,\dots,\theta_k) &= m_1, \\
\mu_2(\theta_1,\dots,\theta_k) &= m_2, \\
&\vdots \\
\mu_k(\theta_1,\dots,\theta_k) &= m_k. \\
\end{aligned}
$$
- In general, it is [not guaranteed]{.orange} that a [solution exist]{.blue} nor its [uniqueness]{.blue}.

## ✏ Example: beta method of moments 

- Let $Y_1,\dots,Y_n$ be an iid random sample from a beta distribution of parameters $\alpha,\beta > 0$ with density 
$$
f(y; \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} y^{\alpha-1}(1 - y)^{\beta-1}, \qquad 0 < y < 1.
$$

- The [moment estimator]{.blue} for $(\alpha, \beta)$ is the ([explicitly available]{.orange}) solution of the system of equations
$$
m_1 = \frac{\alpha}{\alpha + \beta}, \qquad m_2 = \frac{\alpha (\alpha+1)}{(\alpha + \beta) (\alpha + \beta + 1)}.
$$

. . .

- After some algebra we obtain the following relationship, which is a [smooth]{.orange} and [regular]{.orange} function of $(m_1,m_2)$: 
$$
\hat{\alpha} = m_1 \frac{m_1 - m_2}{m_2 - m_1^2}, \qquad \hat{\beta} = (1 - m_1) \frac{m_1 - m_2}{m_2 - m_1^2}.
$$
where $\hat{\sigma}^2 = m_2 - m_1^2$ is the [sample variance]{.blue}. [Remark]{.orange}: what if $m_1 < m_2$? 

## Asymptotics for the MM

- Moments estimators are not necessarily the best estimators, but [under reasonable conditions]{.blue} they are [consistent]{.orange}, they have converge rate $\sqrt{n}$, and they are [asymptotically normal]{.orange}. 

. . .

- We assume the random vector $(Y,Y^2,\dots,Y^r)$ has finite covariance $\Sigma$, then
the multivariate [central limit theorem]{.orange} implies that as $n\rightarrow \infty$
$$
\sqrt{n}\{m - \mu(\theta)\} \overset{d}{\longrightarrow} Z, \qquad Z\sim N_k(0, \Sigma),
$$
where $m = (m_1,\dots,m_k)$ and $\mu(\theta) = (\mu_1(\theta),\dots,\mu_k(\theta))$. 

. . .

- Suppose $\mu(\theta)$ is a [one-to-one]{.blue} mapping and let $g(\mu)$ be the inverse of $\mu(\theta)$, that is $g = \mu^{-1}$. Moreover, suppose that $g$ has [differentiable]{.blue} components $g_r(\cdot)$ for $r = 1,\dots,k$.

- The moments estimator can be written as $\hat{\theta} = g(m)$ and $\theta = g(\mu(\theta))$. Then, as a consequence of the [delta method]{.orange}  the following general result holds:
$$
\sqrt{n}(\hat{\theta} - \theta) \overset{d}{\longrightarrow} Z, \qquad Z\sim N_k\left(0, D \Sigma D^T\right),
$$
where $D = [d_{rr'}]$ is a $k \times k$ matrix whose entries are the derivatives $d_{rr'} = \partial g_r(\mu)/\partial \mu_{r'}$.

## Maximum likelihood estimator

- The method of [maximum likelihood]{.orange} is, by far, the most popular technique for deriving estimators, devloped by [Ronald A. Fisher]{.blue} in Fisher (1922; 1925). 

- Recall that $L(\theta) = L(\theta; \bm{y})$ is the likelihood function and $\ell(\theta) = \log{L(\theta)}$ the log-likelihood. 

::: callout-note
Given a likelihood function $L(\theta)$ for $\theta \in \Theta$, a [maximum likelihood estimate]{.orange} of $\theta$ is an element $\hat{\theta}$ which attains the maximum value of $L(\theta)$ in $\Theta$, i.e. such that
$$
L(\hat{\theta}) = \max_{\theta \in \Theta} L(\theta).
$$


The [maximum likelihood estimator]{.blue} (MLE) of the parameter $\theta$ based on a sample $\bm{Y}$ is $\hat{\theta}(\bm{Y})$.
:::

- Intuitively, the MLE is a reasonable choice: it is the parameter point for which the observed sample is [most likely]{.blue}. 

- Some authors define the maximum likelihood as the [supremum]{.orange} of $L(\theta)$ rather than the maximum, albeit in most regular cases this is not influential. 

## ✏ A regular (and very simple) example


:::incremental

- Let $Y_1,\dots,Y_n$ be a iid random sample from a Poisson distribution of parameter $\lambda > 0$, with [likelihood function]{.orange}
$$
L(\lambda) = \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{y_i}}{y_i!}.
$$

- Therefore the [log-likelihood]{.blue}, up to an additive constant $c$ not depending on $\lambda$, is
$$
\ell(\lambda) = \sum_{i=1}^ny_i\log{\lambda} - n\lambda + c.
$$
- The [maximum likelihood estimator]{.orange} is found by maximizing $\ell(\lambda)$. In this regular problem, this can be done by studying the first derivative:
$$
\ell^*(\lambda) = \frac{1}{\lambda}\sum_{i=1}^ny_i - n.
$$
- We solve $\ell^*(\lambda) = 0$, obtaining the [maximum likelihood estimate]{.blue} $\hat{\lambda} = \bar{y}$. You can check easily this is indeed a maximum. 

:::

## General facts about the MLE

- The MLE may [not exists]{.orange} and is [not]{.orange} necessarily [unique]{.orange}.

- If $s(y)$ is a [sufficient statistic]{.blue}, then the MLE is a function of it.

- The likelihood function has to be maximized in the set space $\Theta$ specified by the statistical model, not over the set of the mathematically admissible values of $\theta$.

- It is not necessary for $\Theta$ to be a numeric set, i.e. we need [not]{.orange} be dealing with a [parametric]{.orange} model, although we shall restrict ourself to this case.

- Often $\hat{\theta}$ cannot be written explicitly as a function of the sample values, i.e. in general the MLE has [no closed-form expression]{.blue}.

. . .

::: callout-warning
#### Theorem (Equivariance)
Let $\psi(\cdot)$ be a one-to-one function (i.e. a [reparametrization]{.blue}) from the set $\Theta$ onto the set $\Psi$. Then the MLE of $\psi = \psi(\theta)$ is $\hat{\psi} = \psi(\hat{\theta})$ where $\hat{\theta}$ denotes the MLE of $\theta$.
:::


## The "regularity conditions"

## Asymptotics for the MLE

## ✏ A non-regular example

## M- and Z- estimators

<!-- ## Plug-in estimators -->

## Bayesian estimators

# Methods of evaluating estimators

# Best unbiased estimators

# Robustness